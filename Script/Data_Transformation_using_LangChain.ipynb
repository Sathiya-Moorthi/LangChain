{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNk3mpw7mr/ZZHaaL0qqrX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Data Transformation**"],"metadata":{"id":"3A_cazRaNPik"}},{"cell_type":"markdown","source":["Text Splitting"],"metadata":{"id":"_tjuJZ8QNSyB"}},{"cell_type":"code","source":["from langchain_community.document_loaders import PyMuPDFLoader\n","\n","pdf_content= PyMuPDFLoader(\"/content/UU_EktaVats_AI_Physics.pdf\")\n","\n","pdf_text= pdf_content.load()"],"metadata":{"collapsed":true,"id":"qrdIQ8qENWKQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pdf_content_list=\"\\n\".join([doc.page_content for doc in pdf_text])\n"],"metadata":{"id":"jOy8lxmhOy6q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Splitting Text using CharacterTextSplitter"],"metadata":{"id":"8u3bWg8cRnJx"}},{"cell_type":"code","source":["from langchain_text_splitters import CharacterTextSplitter"],"metadata":{"collapsed":true,"id":"8i4LfJrHPjYP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1361, chunk_overlap=0)"],"metadata":{"id":"mFUuHEngPwpK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts = text_splitter.split_text(text=pdf_content_list)"],"metadata":{"id":"SyGecdlIQLE1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNPJQcCLQoRq","executionInfo":{"status":"ok","timestamp":1763390658399,"user_tz":-330,"elapsed":95,"user":{"displayName":"Sathiya Moorthi","userId":"16840306610254012468"}},"outputId":"41bb1f14-4108-4aa7-c9b5-b5f54cbe6515","collapsed":true},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['E k t a  Va t s\\nA s s i s t a n t  P r o f e s s o r,  D o c e n t\\nD e p a r t m e n t  o f  I n f o r m a t i o n  Te c h n o l o g y,  U p p s a l a  U n i v e r s i t y  ( U U )\\nB e i j e r  R e s e a r c h e r,  B e i j e r  L a b o r a t o r y  f o r  A I  R e s e a r c h ,  B e i j e r s t i f t e l s e n\\nIntroduction to Large Language Model (LLM)\\nA I  in  Phys ic s  Wor ks h op–  A p r il 11, 2025\\nNavigating in the Artificial Intelligence Era\\nThe future?\\nImage generated using ChatGPT\\nNavigating in the Artificial Intelligence Era\\nThe future?\\nHolographic displays, autonomous bicycles, drone deliveries\\nÅngström à UNGSTROM\\nNavigating in the Artificial Intelligence Era\\nGenerative AI – Generating new content!\\nLearn patterns and structure of input data, and generate new samples that exhibit similar characteristics.\\nArtificial \\nIntelligence\\nGenerative \\nAI\\nAI Taxonomy\\nTechnology that enables computers and \\nmachines to simulate human intelligence \\nand problem-solving capabilities\\nArtificial \\nIntelligence\\nMachine \\nLearning\\nDeep \\nLearning\\nGenerative \\nAI\\nAI Taxonomy\\nSubset of AI, focuses on developing \\nsystems that can learn from data \\nand make decisions based on data\\nAI Evolution Through Decades\\nSource: sk hynix newsroom\\nAI Evolution Through Decades\\n2024\\nThe Nobel Prizes in \\nPhysics and Chemistry \\ngoes to AI!',\n"," 'Source – leewayhertz\\nSynthetic image generation\\nSORA: text-to-video model\\nText:\\nTurn the library into a spaceship.\\nOutput:\\nSource – Open AI, SORA\\nVideo generation!\\nSynthetic image generation\\nImage generation!\\nMidjourney\\nText-to-image models:\\nTwo dogs dressed like roman soldiers on a pirate ship looking at \\nNew York City through a spyglass\\nSource – Open AI, DALL·E -2\\nSource: Twitter, Benjamin Hilton\\nDALL·E\\nAn image of a cat enjoying sprinklers.\\nSource: Microsoft Copilot\\nTTS: Text-to-song; STS: Speech-to-singing \\nSynthetic image generation\\nSource – leewayhertz\\nArtificial \\nIntelligence\\nMachine \\nLearning\\nDeep \\nLearning\\nGenerative AI\\nLLMs\\nAI Taxonomy\\nSubset of ML, uses Artificial Neural Networks \\nto learn from data\\nSubset of AI, focuses on generating new \\ncontent\\nSpecific type of Generative AI model that \\nfocuses on generating human-like text\\nLLMs: Large Language Models\\nLarge Language Model (LLM)\\n• A type of language model\\nLanguage Model\\n• Type of machine learning model trained to predict probability distribution over words\\nVi ses\\nimorgon  \\n0.5\\nsnart \\n \\n0.3 \\npå \\n \\n0.2\\n• Predicts the probability of a word in a sequence based on the previous word\\nLanguage Model\\n• Type of machine learning model trained to predict probability distribution over words\\nVi ses\\nimorgon  \\n0.5\\nsnart \\n \\n0.3 \\npå \\n \\n0.2\\nGoogle search and advanced language models\\nLanguage Model\\n• Classic definition: Probability distribution over a sequence of tokens\\n• For example, if vocabulary of a set of tokens is V = {ate, ball, cheese, mouse, the}, \\na language model p might assign:\\n \\n \\np(!\"#, $%&\\'#, (!#, !\"#, )\"##\\'#) = 0.02,\\n \\n \\np(!\"#, )\"##\\'#, (!#, !\"#, $%&\\'#) = 0.01,\\n \\n \\np($%&\\'#, !\"#, !\"#, )\"##\\'#, (!#) = 0.0001.\\n \\nSource: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\nLanguage Model\\n• Classic definition: Probability distribution over a sequence of tokens\\n• For example, if vocabulary of a set of tokens is V = {ate, ball, cheese, mouse, the}, \\na language model p might assign:\\n \\n \\np(!\"#, $%&\\'#, (!#, !\"#, )\"##\\'#) = 0.02,\\n \\n \\np(!\"#, )\"##\\'#, (!#, !\"#, $%&\\'#) = 0.01,\\n \\n \\np($%&\\'#, !\"#, !\"#, )\"##\\'#, (!#) = 0.0001.\\n• Example: Neural language models - RNNs including LSTMs, Transformers\\n \\n \\np(cheese | (!#, !\"#) = some-neural-network(ate, the, cheese)\\nLarge Language Model (LLM)\\n• A type of language model\\n• Why large: \\n• Trained using massive datasets\\n• With the rise of deep learning and availability of large computational resources, the size of \\nneural language models has increased.\\nImage source: nsc.liu.se\\nBerzelius!\\nLarge Language Model (LLM)\\nSource – cobusgreyling.medium.com\\n2018\\n2023\\n2025: GPT-5?\\nScaling-up – \\nbigger is better?\\nArchitecture\\n• LLM is a type of transformer model\\n• Transformer: \\n• A neural network that learns context and meaning \\nby tracking relationships in sequential data\\nArchitecture\\n• LLM is a type of transformer model\\n• Transformer: \\n• A neural network that learns context and meaning \\nby tracking relationships in sequential data\\n• Encoder-decoder architecture\\n• Encoder extracts features from an input sequence\\n• Decoder uses the features to produce an output \\nsentence\\nHur mår du?\\nHow are you?\\nCase: translation\\n(input)\\n(output)\\nTRANSFORMER\\nArchitecture\\n• LLM is a type of transformer model\\n• Transformer: \\n• A neural network that learns context and meaning \\nby tracking relationships in sequential data\\n• Encoder-decoder architecture\\n• Encoder extracts features from an input sequence\\n• Decoder uses the features to produce an output \\nsentence\\n(input)\\n(output)\\nCan be used independently!\\nTRANSFORMER\\nCase: translation\\nHur mår du?\\nHow are you?\\nTransformer – Recommended Reading\\n• Transformers by Lucas Beyer\\n• Link: https://www.youtube.com/watch?v=EixI6t5oif0\\n• Deep Learning course (1RT720)\\n• Given in period 3\\n• Course responsible: Niklas Wahlström\\n• LLMs & Societal Consequences of AI  (1RT730)\\n• Given in period 1\\n• Course responsible: Ekta Vats\\nTypes of LLMs\\nTypes of LLMs\\n• Encoder only: \\n• Suited for tasks that can understand language, \\n• such as toxicity classification and sentiment analysis. \\n• Example: BERT (Bidirectional Encoder Representations from Transformers)\\nSentiment analysis of tweets\\nTypes of LLMs\\n• Decoder only: \\n• Suited for generating language and content, \\n• such as story writing, blog generation, open-domain Q/A and virtual assistants.\\n• Example: GPT-3 (Generative Pretrained Transformer 3)\\nChatBots',\n"," 'Types of LLMs\\n• Encoder-decoder: \\n• Combine the encoder and decoder components of the transformer architecture\\n• Suited for both understanding and generating content, \\n• such as translation, text-to-code and summarisations. \\n• Example: T5 (Text-to-Text Transformers) by HuggingFace\\nTypes of LLMs\\n• Encoder-decoder: \\n• Example: T5 (Text-to-Text Transformers) by HuggingFace.\\nT5\\n“Translate English to Swedish: Thank you very much.” \\n“Tack så mycket.”\\nTypes of LLMs\\n• Encoder-decoder: \\n• Example: T5 (Text-to-Text Transformers) by HuggingFace.\\nT5\\n“summarize: Sweden became NATO’s newest member on Thursday (7 March 2024), \\nupon depositing its instrument of accession to the North Atlantic Treaty with the \\nGovernment of the United States in Washington DC. With Sweden’s accession, NATO \\nnow counts 32 countries among its members.” \\n“Sweden joined NATO on March 7, 2024, becoming its 32nd member.”\\nLLM Lifecycle\\nLLM Lifecycle\\n1.\\nCollect training data – e.g. Common Crawl\\n2.\\nTrain a LLM – e.g. GPT-3\\n3.\\nAdapt it for downstream tasks – e.g. Q/A\\n4.\\nDeploy LLM to users – e.g. Chatbot\\nImportant\\nData is the fuel!\\nAI is the engine!\\n• To understand and document the composition of your training dataset\\nFurther reading: https://stanford-cs324.github.io/winter2022/lectures/legality/\\nImportant\\n• To understand and document the composition of your training dataset\\n• To be aware of copyright law (IPR, licenses), privacy law, high-risk applications\\n• Is training LLM on this data a copyright/privacy violation?\\n• Can it cause intentional harm – spam, harassment, disinformation, phishing attacks?\\n• Are you deploying LLM in healthcare or education?\\n• Github code and licensing\\nFurther reading: https://stanford-cs324.github.io/winter2022/lectures/legality/\\nMultimodal LLMs\\nMultimodal LLMs\\n• Unimodal LLMs are trained on a single modality of data, such as text\\n• Lack visual context\\nMultimodal LLMs\\n• Unimodal LLMs are trained on a single modality of data, such as text\\n• Lack visual context\\n• Multimodal LLMs: \\n• Understand and generate content across multiple modalities,\\n• such as text, images, audio, video, or other sensor data\\n• Vision-language models: \\n• Multimodal models that can learn from images and text\\nVision-Language Models\\n• Main idea: Unify the image and text representation, and feed it to a textual decoder for \\ngeneration\\nVision-Language Models\\n• Main idea: Unify the image and text representation, and feed it to a textual decoder for \\ngeneration\\n• Typically consists of two main components: \\n• Vision encoder: extracts image features and encodes them into a format that can be understood \\nby the language decoder. Example: ResNet, ViT\\n• Language decoder: takes these encoded visual features along with any textual input and \\ngenerates descriptions, or captions, etc. Example: GPT-2, GPT-3\\nVision-Language Models\\nhttps://huggingface.co/blog/vlms\\nVision-Language Models\\nhttps://huggingface.co/blog/vlms\\nContrastive Language-Image Pretraining (CLIP)\\n• CLIP differs from traditional vision-language models \\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.\\nContrastive Language-Image Pretraining (CLIP)\\n• CLIP differs from traditional vision-language models \\n• It does not generate text descriptions or captions for images\\n• Focuses on learning a joint representation space where images and text can be compared directly\\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.\\nContrastive Language-Image Pretraining (CLIP)\\n• CLIP differs from traditional vision-language models \\n• It does not generate text descriptions or captions for images\\n• Focuses on learning a joint representation space where images and text can be compared directly\\n• Enables various downstream tasks, such as \\n• Zero-shot image classification: classify images into one of several classes, without any prior \\ntraining or knowledge of the classes\\n• Zero-shot image retrieval, text-based image generation\\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.\\nContrastive Language-Image Pretraining (CLIP)\\n1.\\nJointly trains a text encoder and an image encoder to predict the correct image—text pair\\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.\\nContrastive Language-Image Pretraining (CLIP)\\n1.\\nJointly trains a text encoder and an image encoder to predict the correct image—text pair\\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.\\nContrastive Pre-training\\nText embeddings\\nImage embeddings\\nTransformer\\n(ResNet or ViT)\\nContrastive learning framework\\n•\\nMaximize the cosine similarities between \\ncorrect image-text pairs\\n•\\nMinimize the cosine similarities for \\ndissimilar pairs (non-diagonal elements)\\nContrastive Language-Image Pretraining (CLIP)\\n2.\\nConverts training dataset classes into captions\\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.\\nContrastive Language-Image Pretraining (CLIP)\\n3.\\nEstimates the best caption for the given input image for zero-shot prediction\\n• Calculate similarity between an image vector and multiple caption vectors, selecting the \\none with the highest score\\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.\\nContrastive Language-Image Pretraining (CLIP)\\n3.\\nEstimates the best caption for the given input image for zero-shot prediction\\n• Calculate similarity between an image vector and multiple caption vectors, selecting the \\none with the highest score\\n•\\nTrained on 400M image-text pairs\\n•\\nCLIP can perform image tasks using only text, no extra training needed\\n•\\nCLIP encodings + decoder models (e.g. GPT-2) => image captioning\\nSome Interesting Examples\\nModern Handwriting Recognition using ChatGPT\\nSource: Medium article on Exploring Multimodal Large Language Models: A Step Forward in AI\\nPrompt: Can you recognise this text?\\nModern Handwriting Recognition using ChatGPT\\nSource: Medium article on Exploring Multimodal Large Language Models: A Step Forward in AI\\nPrompt: Can you recognise this text?\\nOld Handwriting Recognition using ChatGPT\\nSource: Medium article on Exploring Multimodal Large Language Models: A Step Forward in AI\\nPrompt: Can you read this?\\nOld Handwriting Recognition using ChatGPT\\nSource: Medium article on Exploring Multimodal Large Language Models: A Step Forward in AI\\nPrompt: Can you read this?\\nAncient Handwriting Recognition using Microsoft Copilot\\nNota bene is the Latin phrase meaning note well\\nOCR and Document Question Answering\\n• Text…\\nhttps://huggingface.co/docs/transformers/main/en/tasks/document_question_answering\\nQuestion: Who is in cc in this letter? \\nAnswer: T.F. Riehl\\nUnderstanding Complex Parking Signs – ChatGPT\\nSource: Medium article on Exploring Multimodal Large Language Models: A Step Forward in AI\\nPrompt: Suppose it is Wednesday and the time is 4PM. Am I allowed to park my car at this spot?\\nAnyone?\\nUnderstanding Complex Parking Signs – GPT-4V\\nSource: Medium article on Exploring Multimodal Large Language Models: A Step Forward in AI\\nPrompt: Suppose it is Wednesday and the time is 4PM. Am I allowed to park my car at this spot?\\nVisual Question Answering\\nMarino, Kenneth, et al. \"Ok-vqa: A visual question answering benchmark requiring external knowledge.\" Proceedings of the IEEE/cvf conference on computer vision and pattern recognition. 2019.\\nVisual Question Answering\\nLiu, Haotian, et al. \"Visual instruction tuning.\" Advances in neural information processing systems 36 (2024).\\nLLaVA: Large Language and Vision Assistant - CLIP visual encoder + Vicuna LLM\\nVisual Question Answering\\nSource: Medium article on Exploring Multimodal Large Language Models: A Step Forward in AI\\nLLaVA: Large Language and Vision Assistant - CLIP visual encoder + Vicuna LLM\\nWhisper by OpenAI\\n• Speech-to-text model, performs:\\n• Speech recognition\\n• Speech translation\\n• Spoken language identification\\n• Voice activity detection\\nFocus: Swedish speech\\nImage source: LinkedIn\\nLLMs and Video Analysis\\nPlatform: Azure AI\\nChallenges and limitations\\nChallenges and Limitations\\n• Out-of-date training data\\nThis example was tested on 10 March, 2025\\nChallenges and Limitations\\n• Hallucinations\\n•\\nFacts are sometimes extrapolated, \\n•\\nLLMs try to invent facts, \\n•\\narticulating the inaccurate information in a convincing way.\\nChallenges and Limitations\\n• Hallucinations\\n•\\nFacts are sometimes extrapolated, \\n•\\nLLMs try to invent facts, \\n•\\narticulating the inaccurate information in a convincing way.\\nThis example was tested in Spring 2024\\nChallenges and Limitations\\n• Hallucinations\\n•\\nFacts are sometimes extrapolated, \\n•\\nLLMs try to invent facts, \\n•\\narticulating the inaccurate information in a convincing way.\\nTested: October 2024\\nChallenges and Limitations\\nTested: March 2025\\nChallenges and Limitations\\n• Hallucinations\\nChallenges and Limitations\\n• Hallucinations\\nWhat is the correct answer?\\nChallenges and Limitations\\n• Hallucinations\\n• It is a language model!\\n• Training data lacks focus on math concepts and problem-solving.\\n5,162,060\\nHallucinations, Case – Law\\nThis example was shared by a lawyer\\nHallucinations, Case – Law\\nThis example was shared by a lawyer\\nHallucinations, Case – Law\\nMicrosoft Copilot did not hallucinate!\\nHallucinations, Case – MiniGPT-4\\nSource: MiniGPT-4 by HuggingFace\\nHallucinations, Case – MiniGPT-4\\nSource: MiniGPT-4 by HuggingFace\\nHallucinations, Case - HTRFlow\\nHTRFlow model\\nBevittna  \\n(Bevittna translates to witness)\\nSource: https://huggingface.co/spaces/Riksarkivet/htr_demo (2023)\\nPotential Solution: Retrieval Augmented Generation (RAG)\\n• Helps address both hallucinations and out-of-date training data issues\\nRetrieval Augmented Generation (RAG)\\n• Build LLMs with current and reliable information\\n• Extending its utility to specific data sources\\n• Allows LLMs to go beyond their knowledge-base, enabling them to access real-time \\ndata, and providing up to date responses\\n• Example use case: Improve Math Q/A\\nRetrieval Augmented Generation (RAG)\\nRetrieval Augmented Generation (RAG)\\nThis example was on 10 March, 2025\\nRAG Example Projects from UU course on LLMs\\n• Teaching LLM to teach AI\\nRAG Example Projects from UU course on LLMs\\n• Teaching LLM to teach AI \\n• Chat with 1177.se\\nRAG Example Projects from UU course on LLMs\\n• Teaching LLM to teach AI \\n• Chat with 1177.se\\n• LLM powered teaching assistant for Smarter Education\\n• Exercise sheet generation\\n• Based on age, grade and interest of the child\\nChallenges and Limitations\\n• Bias and misinformation as ethical concerns\\nSocial Biases in Language Models: http://uu.diva-portal.org/smash/get/diva2:1696604/FULLTEXT01.pdf\\nChallenges and Limitations\\n• Bias and misinformation as ethical concerns\\n• Key pointer: \\n• LLM has inherited society\\'s stereotypes due to the training data being fed into it.\\n• Other cases: voice assistants and FaceID\\nVoice Assistants and Accent Bias\\nSource: https://www.amazon.science/blog/how-alexa-learned-to-speak-with-an-irish-accent (2023)\\n• Data problem!\\n• The higher the quantity and diversity of speech samples in a corpus, the more accurate \\nthe resulting model\\nAmazon researchers improved Irish-accented training data by using a voice conversion model\\nFace ID and Limitations\\nSource: https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212  (2018)\\nFace ID and Limitations\\nSource: https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212  (2018)\\nFace ID and Limitations\\n• This bias arise from the imbalance in the training data\\n• Lighting conditions: Darker skin tones might reflect less light, potentially affecting \\nrecognition accuracy\\nFace ID and Limitations\\n• This bias arise from the imbalance in the training data\\n• Lighting conditions: Darker skin tones might reflect less light, potentially affecting \\nrecognition accuracy\\n• Modern face recognition systems:\\n• Diverse training data\\n• Algorithmic improvements \\n• Aim to achieve robust performance, regardless of skin tone or lighting conditions.\\n• Biases can still persist\\nFuture Insights!\\nFuture Insights!\\nScaling-up – bigger is better?\\n•\\nQuantitatively: different capabilities\\n•\\nQualitatively: different societal impact\\nFuture Insights!\\nScaling-up – bigger is better?\\nIn-Context Learning\\nhttps://www.cs.princeton.edu/courses/archive/fall22/cos597G/lectures/lec04.pdf\\nZero-shot\\nOne-shot\\nFew-shot\\nFuture Insights!\\nScaling-up – bigger is better?\\nIn-Context Learning\\nAI powered precision in healthcare\\nAnalyse medical images, and using LLMs correlate medical findings with patient \\nhistory, delivering comprehensive diagnostics and potential treatment options. \\nAlso… Applications in Physics!\\nAdvanced Multimodal LLMs\\n• Example: NExT-GPT (Any-to-Any Multimodal LLM)\\n• Text + Video à Text + Image\\nhttps://next-gpt.github.io\\nImageBind by Meta\\n6 modalities:\\nImages/videos,\\nAudio, Text,\\nDepth, Thermal,\\nInertial measurement units (IMUs)\\n+ \\nDiffusion Models (generation)\\nYou can select “off” for this option, \\nin your ChatGPT settings.\\nWe conducted an exam for \\n• Total 10 questions\\n• Following are our findings:\\n• Both excel in creative thinking, image generation and summarization. \\n• ChatGPT struggles with legal reasoning but excels in math. \\n• Neither handles old handwriting well.\\n• Overall, score for Copilot was higher.\\n• Curious to know how it works in Physics…\\nLink\\nThank you!\\nEmail    : ekta.vats@it.uu.se\\nWebpage: https://www.ektavats.se']"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["Splitting Text using RecursiveCharacterTextSplitter"],"metadata":{"id":"mBfBdObqRt7A"}},{"cell_type":"code","source":["from langchain_text_splitters import RecursiveCharacterTextSplitter"],"metadata":{"id":"yqI6Z1CMRhUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_splitter = RecursiveCharacterTextSplitter(chunk_size=1361,chunk_overlap=0)"],"metadata":{"id":"zEsg29bBR1VL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts_recursive = text_splitter.split_text(text=pdf_content_list)"],"metadata":{"id":"hM2qzJ6NSFgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts_recursive"],"metadata":{"id":"anDj4e1CScqk","executionInfo":{"status":"ok","timestamp":1763390907963,"user_tz":-330,"elapsed":18,"user":{"displayName":"Sathiya Moorthi","userId":"16840306610254012468"}},"outputId":"8e5cf3c0-758b-4064-f62f-cdf2bb589910","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['E k t a  Va t s\\nA s s i s t a n t  P r o f e s s o r,  D o c e n t\\nD e p a r t m e n t  o f  I n f o r m a t i o n  Te c h n o l o g y,  U p p s a l a  U n i v e r s i t y  ( U U )\\nB e i j e r  R e s e a r c h e r,  B e i j e r  L a b o r a t o r y  f o r  A I  R e s e a r c h ,  B e i j e r s t i f t e l s e n\\nIntroduction to Large Language Model (LLM)\\nA I  in  Phys ic s  Wor ks h op–  A p r il 11, 2025\\nNavigating in the Artificial Intelligence Era\\nThe future?\\nImage generated using ChatGPT\\nNavigating in the Artificial Intelligence Era\\nThe future?\\nHolographic displays, autonomous bicycles, drone deliveries\\nÅngström à UNGSTROM\\nNavigating in the Artificial Intelligence Era\\nGenerative AI – Generating new content!\\nLearn patterns and structure of input data, and generate new samples that exhibit similar characteristics.\\nArtificial \\nIntelligence\\nGenerative \\nAI\\nAI Taxonomy\\nTechnology that enables computers and \\nmachines to simulate human intelligence \\nand problem-solving capabilities\\nArtificial \\nIntelligence\\nMachine \\nLearning\\nDeep \\nLearning\\nGenerative \\nAI\\nAI Taxonomy\\nSubset of AI, focuses on developing \\nsystems that can learn from data \\nand make decisions based on data\\nAI Evolution Through Decades\\nSource: sk hynix newsroom\\nAI Evolution Through Decades\\n2024\\nThe Nobel Prizes in \\nPhysics and Chemistry \\ngoes to AI!',\n"," 'Source – leewayhertz\\nSynthetic image generation\\nSORA: text-to-video model\\nText:\\nTurn the library into a spaceship.\\nOutput:\\nSource – Open AI, SORA\\nVideo generation!\\nSynthetic image generation\\nImage generation!\\nMidjourney\\nText-to-image models:\\nTwo dogs dressed like roman soldiers on a pirate ship looking at \\nNew York City through a spyglass\\nSource – Open AI, DALL·E -2\\nSource: Twitter, Benjamin Hilton\\nDALL·E\\nAn image of a cat enjoying sprinklers.\\nSource: Microsoft Copilot\\nTTS: Text-to-song; STS: Speech-to-singing \\nSynthetic image generation\\nSource – leewayhertz\\nArtificial \\nIntelligence\\nMachine \\nLearning\\nDeep \\nLearning\\nGenerative AI\\nLLMs\\nAI Taxonomy\\nSubset of ML, uses Artificial Neural Networks \\nto learn from data\\nSubset of AI, focuses on generating new \\ncontent\\nSpecific type of Generative AI model that \\nfocuses on generating human-like text\\nLLMs: Large Language Models\\nLarge Language Model (LLM)\\n• A type of language model\\nLanguage Model\\n• Type of machine learning model trained to predict probability distribution over words\\nVi ses\\nimorgon  \\n0.5\\nsnart \\n \\n0.3 \\npå \\n \\n0.2\\n• Predicts the probability of a word in a sequence based on the previous word\\nLanguage Model\\n• Type of machine learning model trained to predict probability distribution over words\\nVi ses\\nimorgon  \\n0.5\\nsnart \\n \\n0.3 \\npå \\n \\n0.2\\nGoogle search and advanced language models',\n"," 'Language Model\\n• Classic definition: Probability distribution over a sequence of tokens\\n• For example, if vocabulary of a set of tokens is V = {ate, ball, cheese, mouse, the}, \\na language model p might assign:\\n \\n \\np(!\"#, $%&\\'#, (!#, !\"#, )\"##\\'#) = 0.02,\\n \\n \\np(!\"#, )\"##\\'#, (!#, !\"#, $%&\\'#) = 0.01,\\n \\n \\np($%&\\'#, !\"#, !\"#, )\"##\\'#, (!#) = 0.0001.\\n \\nSource: https://stanford-cs324.github.io/winter2022/lectures/introduction/\\nLanguage Model\\n• Classic definition: Probability distribution over a sequence of tokens\\n• For example, if vocabulary of a set of tokens is V = {ate, ball, cheese, mouse, the}, \\na language model p might assign:\\n \\n \\np(!\"#, $%&\\'#, (!#, !\"#, )\"##\\'#) = 0.02,\\n \\n \\np(!\"#, )\"##\\'#, (!#, !\"#, $%&\\'#) = 0.01,\\n \\n \\np($%&\\'#, !\"#, !\"#, )\"##\\'#, (!#) = 0.0001.\\n• Example: Neural language models - RNNs including LSTMs, Transformers\\n \\n \\np(cheese | (!#, !\"#) = some-neural-network(ate, the, cheese)\\nLarge Language Model (LLM)\\n• A type of language model\\n• Why large: \\n• Trained using massive datasets\\n• With the rise of deep learning and availability of large computational resources, the size of \\nneural language models has increased.\\nImage source: nsc.liu.se\\nBerzelius!\\nLarge Language Model (LLM)\\nSource – cobusgreyling.medium.com\\n2018\\n2023\\n2025: GPT-5?\\nScaling-up – \\nbigger is better?\\nArchitecture\\n• LLM is a type of transformer model\\n• Transformer:',\n"," '• A neural network that learns context and meaning \\nby tracking relationships in sequential data\\nArchitecture\\n• LLM is a type of transformer model\\n• Transformer: \\n• A neural network that learns context and meaning \\nby tracking relationships in sequential data\\n• Encoder-decoder architecture\\n• Encoder extracts features from an input sequence\\n• Decoder uses the features to produce an output \\nsentence\\nHur mår du?\\nHow are you?\\nCase: translation\\n(input)\\n(output)\\nTRANSFORMER\\nArchitecture\\n• LLM is a type of transformer model\\n• Transformer: \\n• A neural network that learns context and meaning \\nby tracking relationships in sequential data\\n• Encoder-decoder architecture\\n• Encoder extracts features from an input sequence\\n• Decoder uses the features to produce an output \\nsentence\\n(input)\\n(output)\\nCan be used independently!\\nTRANSFORMER\\nCase: translation\\nHur mår du?\\nHow are you?\\nTransformer – Recommended Reading\\n• Transformers by Lucas Beyer\\n• Link: https://www.youtube.com/watch?v=EixI6t5oif0\\n• Deep Learning course (1RT720)\\n• Given in period 3\\n• Course responsible: Niklas Wahlström\\n• LLMs & Societal Consequences of AI  (1RT730)\\n• Given in period 1\\n• Course responsible: Ekta Vats\\nTypes of LLMs\\nTypes of LLMs\\n• Encoder only: \\n• Suited for tasks that can understand language, \\n• such as toxicity classification and sentiment analysis.',\n"," '• Example: BERT (Bidirectional Encoder Representations from Transformers)\\nSentiment analysis of tweets\\nTypes of LLMs\\n• Decoder only: \\n• Suited for generating language and content, \\n• such as story writing, blog generation, open-domain Q/A and virtual assistants.\\n• Example: GPT-3 (Generative Pretrained Transformer 3)\\nChatBots',\n"," 'Types of LLMs\\n• Encoder-decoder: \\n• Combine the encoder and decoder components of the transformer architecture\\n• Suited for both understanding and generating content, \\n• such as translation, text-to-code and summarisations. \\n• Example: T5 (Text-to-Text Transformers) by HuggingFace\\nTypes of LLMs\\n• Encoder-decoder: \\n• Example: T5 (Text-to-Text Transformers) by HuggingFace.\\nT5\\n“Translate English to Swedish: Thank you very much.” \\n“Tack så mycket.”\\nTypes of LLMs\\n• Encoder-decoder: \\n• Example: T5 (Text-to-Text Transformers) by HuggingFace.\\nT5\\n“summarize: Sweden became NATO’s newest member on Thursday (7 March 2024), \\nupon depositing its instrument of accession to the North Atlantic Treaty with the \\nGovernment of the United States in Washington DC. With Sweden’s accession, NATO \\nnow counts 32 countries among its members.” \\n“Sweden joined NATO on March 7, 2024, becoming its 32nd member.”\\nLLM Lifecycle\\nLLM Lifecycle\\n1.\\nCollect training data – e.g. Common Crawl\\n2.\\nTrain a LLM – e.g. GPT-3\\n3.\\nAdapt it for downstream tasks – e.g. Q/A\\n4.\\nDeploy LLM to users – e.g. Chatbot\\nImportant\\nData is the fuel!\\nAI is the engine!\\n• To understand and document the composition of your training dataset\\nFurther reading: https://stanford-cs324.github.io/winter2022/lectures/legality/\\nImportant\\n• To understand and document the composition of your training dataset',\n"," '• To be aware of copyright law (IPR, licenses), privacy law, high-risk applications\\n• Is training LLM on this data a copyright/privacy violation?\\n• Can it cause intentional harm – spam, harassment, disinformation, phishing attacks?\\n• Are you deploying LLM in healthcare or education?\\n• Github code and licensing\\nFurther reading: https://stanford-cs324.github.io/winter2022/lectures/legality/\\nMultimodal LLMs\\nMultimodal LLMs\\n• Unimodal LLMs are trained on a single modality of data, such as text\\n• Lack visual context\\nMultimodal LLMs\\n• Unimodal LLMs are trained on a single modality of data, such as text\\n• Lack visual context\\n• Multimodal LLMs: \\n• Understand and generate content across multiple modalities,\\n• such as text, images, audio, video, or other sensor data\\n• Vision-language models: \\n• Multimodal models that can learn from images and text\\nVision-Language Models\\n• Main idea: Unify the image and text representation, and feed it to a textual decoder for \\ngeneration\\nVision-Language Models\\n• Main idea: Unify the image and text representation, and feed it to a textual decoder for \\ngeneration\\n• Typically consists of two main components: \\n• Vision encoder: extracts image features and encodes them into a format that can be understood \\nby the language decoder. Example: ResNet, ViT',\n"," '• Language decoder: takes these encoded visual features along with any textual input and \\ngenerates descriptions, or captions, etc. Example: GPT-2, GPT-3\\nVision-Language Models\\nhttps://huggingface.co/blog/vlms\\nVision-Language Models\\nhttps://huggingface.co/blog/vlms\\nContrastive Language-Image Pretraining (CLIP)\\n• CLIP differs from traditional vision-language models \\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.\\nContrastive Language-Image Pretraining (CLIP)\\n• CLIP differs from traditional vision-language models \\n• It does not generate text descriptions or captions for images\\n• Focuses on learning a joint representation space where images and text can be compared directly\\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.\\nContrastive Language-Image Pretraining (CLIP)\\n• CLIP differs from traditional vision-language models \\n• It does not generate text descriptions or captions for images\\n• Focuses on learning a joint representation space where images and text can be compared directly\\n• Enables various downstream tasks, such as \\n• Zero-shot image classification: classify images into one of several classes, without any prior',\n"," 'training or knowledge of the classes\\n• Zero-shot image retrieval, text-based image generation\\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.\\nContrastive Language-Image Pretraining (CLIP)\\n1.\\nJointly trains a text encoder and an image encoder to predict the correct image—text pair\\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.\\nContrastive Language-Image Pretraining (CLIP)\\n1.\\nJointly trains a text encoder and an image encoder to predict the correct image—text pair\\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.\\nContrastive Pre-training\\nText embeddings\\nImage embeddings\\nTransformer\\n(ResNet or ViT)\\nContrastive learning framework\\n•\\nMaximize the cosine similarities between \\ncorrect image-text pairs\\n•\\nMinimize the cosine similarities for \\ndissimilar pairs (non-diagonal elements)\\nContrastive Language-Image Pretraining (CLIP)\\n2.\\nConverts training dataset classes into captions\\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.',\n"," 'Contrastive Language-Image Pretraining (CLIP)\\n3.\\nEstimates the best caption for the given input image for zero-shot prediction\\n• Calculate similarity between an image vector and multiple caption vectors, selecting the \\none with the highest score\\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.\\nContrastive Language-Image Pretraining (CLIP)\\n3.\\nEstimates the best caption for the given input image for zero-shot prediction\\n• Calculate similarity between an image vector and multiple caption vectors, selecting the \\none with the highest score\\n•\\nTrained on 400M image-text pairs\\n•\\nCLIP can perform image tasks using only text, no extra training needed\\n•\\nCLIP encodings + decoder models (e.g. GPT-2) => image captioning\\nSome Interesting Examples\\nModern Handwriting Recognition using ChatGPT\\nSource: Medium article on Exploring Multimodal Large Language Models: A Step Forward in AI\\nPrompt: Can you recognise this text?\\nModern Handwriting Recognition using ChatGPT\\nSource: Medium article on Exploring Multimodal Large Language Models: A Step Forward in AI\\nPrompt: Can you recognise this text?\\nOld Handwriting Recognition using ChatGPT\\nSource: Medium article on Exploring Multimodal Large Language Models: A Step Forward in AI\\nPrompt: Can you read this?',\n"," 'Old Handwriting Recognition using ChatGPT\\nSource: Medium article on Exploring Multimodal Large Language Models: A Step Forward in AI\\nPrompt: Can you read this?\\nAncient Handwriting Recognition using Microsoft Copilot\\nNota bene is the Latin phrase meaning note well\\nOCR and Document Question Answering\\n• Text…\\nhttps://huggingface.co/docs/transformers/main/en/tasks/document_question_answering\\nQuestion: Who is in cc in this letter? \\nAnswer: T.F. Riehl\\nUnderstanding Complex Parking Signs – ChatGPT\\nSource: Medium article on Exploring Multimodal Large Language Models: A Step Forward in AI\\nPrompt: Suppose it is Wednesday and the time is 4PM. Am I allowed to park my car at this spot?\\nAnyone?\\nUnderstanding Complex Parking Signs – GPT-4V\\nSource: Medium article on Exploring Multimodal Large Language Models: A Step Forward in AI\\nPrompt: Suppose it is Wednesday and the time is 4PM. Am I allowed to park my car at this spot?\\nVisual Question Answering\\nMarino, Kenneth, et al. \"Ok-vqa: A visual question answering benchmark requiring external knowledge.\" Proceedings of the IEEE/cvf conference on computer vision and pattern recognition. 2019.\\nVisual Question Answering\\nLiu, Haotian, et al. \"Visual instruction tuning.\" Advances in neural information processing systems 36 (2024).\\nLLaVA: Large Language and Vision Assistant - CLIP visual encoder + Vicuna LLM',\n"," 'Visual Question Answering\\nSource: Medium article on Exploring Multimodal Large Language Models: A Step Forward in AI\\nLLaVA: Large Language and Vision Assistant - CLIP visual encoder + Vicuna LLM\\nWhisper by OpenAI\\n• Speech-to-text model, performs:\\n• Speech recognition\\n• Speech translation\\n• Spoken language identification\\n• Voice activity detection\\nFocus: Swedish speech\\nImage source: LinkedIn\\nLLMs and Video Analysis\\nPlatform: Azure AI\\nChallenges and limitations\\nChallenges and Limitations\\n• Out-of-date training data\\nThis example was tested on 10 March, 2025\\nChallenges and Limitations\\n• Hallucinations\\n•\\nFacts are sometimes extrapolated, \\n•\\nLLMs try to invent facts, \\n•\\narticulating the inaccurate information in a convincing way.\\nChallenges and Limitations\\n• Hallucinations\\n•\\nFacts are sometimes extrapolated, \\n•\\nLLMs try to invent facts, \\n•\\narticulating the inaccurate information in a convincing way.\\nThis example was tested in Spring 2024\\nChallenges and Limitations\\n• Hallucinations\\n•\\nFacts are sometimes extrapolated, \\n•\\nLLMs try to invent facts, \\n•\\narticulating the inaccurate information in a convincing way.\\nTested: October 2024\\nChallenges and Limitations\\nTested: March 2025\\nChallenges and Limitations\\n• Hallucinations\\nChallenges and Limitations\\n• Hallucinations\\nWhat is the correct answer?\\nChallenges and Limitations\\n• Hallucinations',\n"," '• It is a language model!\\n• Training data lacks focus on math concepts and problem-solving.\\n5,162,060\\nHallucinations, Case – Law\\nThis example was shared by a lawyer\\nHallucinations, Case – Law\\nThis example was shared by a lawyer\\nHallucinations, Case – Law\\nMicrosoft Copilot did not hallucinate!\\nHallucinations, Case – MiniGPT-4\\nSource: MiniGPT-4 by HuggingFace\\nHallucinations, Case – MiniGPT-4\\nSource: MiniGPT-4 by HuggingFace\\nHallucinations, Case - HTRFlow\\nHTRFlow model\\nBevittna  \\n(Bevittna translates to witness)\\nSource: https://huggingface.co/spaces/Riksarkivet/htr_demo (2023)\\nPotential Solution: Retrieval Augmented Generation (RAG)\\n• Helps address both hallucinations and out-of-date training data issues\\nRetrieval Augmented Generation (RAG)\\n• Build LLMs with current and reliable information\\n• Extending its utility to specific data sources\\n• Allows LLMs to go beyond their knowledge-base, enabling them to access real-time \\ndata, and providing up to date responses\\n• Example use case: Improve Math Q/A\\nRetrieval Augmented Generation (RAG)\\nRetrieval Augmented Generation (RAG)\\nThis example was on 10 March, 2025\\nRAG Example Projects from UU course on LLMs\\n• Teaching LLM to teach AI\\nRAG Example Projects from UU course on LLMs\\n• Teaching LLM to teach AI \\n• Chat with 1177.se\\nRAG Example Projects from UU course on LLMs\\n• Teaching LLM to teach AI',\n"," \"• Chat with 1177.se\\n• LLM powered teaching assistant for Smarter Education\\n• Exercise sheet generation\\n• Based on age, grade and interest of the child\\nChallenges and Limitations\\n• Bias and misinformation as ethical concerns\\nSocial Biases in Language Models: http://uu.diva-portal.org/smash/get/diva2:1696604/FULLTEXT01.pdf\\nChallenges and Limitations\\n• Bias and misinformation as ethical concerns\\n• Key pointer: \\n• LLM has inherited society's stereotypes due to the training data being fed into it.\\n• Other cases: voice assistants and FaceID\\nVoice Assistants and Accent Bias\\nSource: https://www.amazon.science/blog/how-alexa-learned-to-speak-with-an-irish-accent (2023)\\n• Data problem!\\n• The higher the quantity and diversity of speech samples in a corpus, the more accurate \\nthe resulting model\\nAmazon researchers improved Irish-accented training data by using a voice conversion model\\nFace ID and Limitations\\nSource: https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212  (2018)\\nFace ID and Limitations\\nSource: https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212  (2018)\\nFace ID and Limitations\\n• This bias arise from the imbalance in the training data\\n• Lighting conditions: Darker skin tones might reflect less light, potentially affecting \\nrecognition accuracy\",\n"," 'Face ID and Limitations\\n• This bias arise from the imbalance in the training data\\n• Lighting conditions: Darker skin tones might reflect less light, potentially affecting \\nrecognition accuracy\\n• Modern face recognition systems:\\n• Diverse training data\\n• Algorithmic improvements \\n• Aim to achieve robust performance, regardless of skin tone or lighting conditions.\\n• Biases can still persist\\nFuture Insights!\\nFuture Insights!\\nScaling-up – bigger is better?\\n•\\nQuantitatively: different capabilities\\n•\\nQualitatively: different societal impact\\nFuture Insights!\\nScaling-up – bigger is better?\\nIn-Context Learning\\nhttps://www.cs.princeton.edu/courses/archive/fall22/cos597G/lectures/lec04.pdf\\nZero-shot\\nOne-shot\\nFew-shot\\nFuture Insights!\\nScaling-up – bigger is better?\\nIn-Context Learning\\nAI powered precision in healthcare\\nAnalyse medical images, and using LLMs correlate medical findings with patient \\nhistory, delivering comprehensive diagnostics and potential treatment options. \\nAlso… Applications in Physics!\\nAdvanced Multimodal LLMs\\n• Example: NExT-GPT (Any-to-Any Multimodal LLM)\\n• Text + Video à Text + Image\\nhttps://next-gpt.github.io\\nImageBind by Meta\\n6 modalities:\\nImages/videos,\\nAudio, Text,\\nDepth, Thermal,\\nInertial measurement units (IMUs)\\n+ \\nDiffusion Models (generation)\\nYou can select “off” for this option, \\nin your ChatGPT settings.',\n"," 'We conducted an exam for \\n• Total 10 questions\\n• Following are our findings:\\n• Both excel in creative thinking, image generation and summarization. \\n• ChatGPT struggles with legal reasoning but excels in math. \\n• Neither handles old handwriting well.\\n• Overall, score for Copilot was higher.\\n• Curious to know how it works in Physics…\\nLink\\nThank you!\\nEmail    : ekta.vats@it.uu.se\\nWebpage: https://www.ektavats.se']"]},"metadata":{},"execution_count":23}]}]}